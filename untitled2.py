# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15mWVW0BG7OQPLRKHxYddxr9KB66-SiuF
"""

!pip install PyPDF2
!pip install pdfplumber
import transformers
import torch
import PyPDF2
import pdfplumber
import json
import re

# Initialize the model and pipeline
model_id = "WiroAI/WiroAI-Finance-Qwen-1.5B"
pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)
pipeline.model.eval()

from google.colab import drive
drive.mount('/content/drive')

# Read PDF page by page

def read_pdf(file_path):
    """Extracts text from a PDF file page by page."""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:  # Skip empty pages
                yield text + "\n"

#need to tinker the prompt more for quality or change to a bigger llm , need  more compute

def extract_entities(text, chunk_size=500):

    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    all_entities = []

    for chunk in chunks:
        # Prompt to guide LLM extraction
        prompt = f"""
        read all the file and extract selected datapoints from the following financial entities from the pdfs data:
        Extract structured financial data in JSON format.only add data of companies with these  Required fields:
        - "Company Name"
        - "Report Date"
        - "Profit Before Tax (PBT)"
        - "Bonus data about the company like- Declared Interim Dividend, Investment in Subsidiary,etc" (if available)


        Extract from this text:
        \"\"\"
        {chunk}
        \"\"\"
        """

        # Generate the response from the model
        response = pipeline(
            prompt,
            max_new_tokens=512,
            eos_token_id=pipeline.tokenizer.eos_token_id,
            do_sample=True,
            temperature=0.9,
        )

        generated_text = response[0]["generated_text"]

        # Parse the generated JSON text
        try:
            # Use regular expression to extract JSON object from the generated text
            json_str = re.search(r'\{.*\}', generated_text, re.DOTALL).group()
            entities = json.loads(json_str)
            all_entities.update(entities) # Merge dictionaries

        except (json.JSONDecodeError, AttributeError):
            # If parsing fails, return an empty dictionary or handle accordingly
            pass

    return all_entities

# Process PDFs and save results
def process_pdfs(pdf_paths):
    all_entities = []
    for pdf_path in pdf_paths:
        # Process each page individually
        for page_text in read_pdf(pdf_path):
            entities = extract_entities(page_text)
            all_entities.append(entities)
    return all_entities

def save_as_json(data, output_path):
    with open(output_path, "w") as file:
        json.dump(data, file, indent=4)

# Adjust file paths to the correct locations on Google Drive
pdf_paths = [
    "/content/drive/My Drive/financial_pdfs/1_FinancialResults_05022025142214.pdf",
    "/content/drive/My Drive/financial_pdfs/Amaar raja Earnings Summary.pdf"
]

extracted_data = process_pdfs(pdf_paths)
save_as_json(extracted_data, "/content/drive/My Drive/financial_pdfs/output.json")

print("Financial data extraction complete. Results saved to output.json")

import json
import os


input_file = "/content/drive/My Drive/financial_pdfs/output.json"  # raw extracted data
cleaned_file = "/content/drive/My Drive/financial_pdfs/cleaned.json"  # Output file (cleaned data)

# Step 1: Load JSON Data from output.json
def load_json(file_path):
    """Loads JSON data from a file."""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            data = json.load(file)
        print(f"üìÇ Loaded {len(data)} records from {file_path}")
        return data
    except FileNotFoundError:
        print(f"‚ùå Error: File not found - {file_path}")
        return []
    except json.JSONDecodeError:
        print(f"‚ùå Error: Invalid JSON format in {file_path}")
        return []

# Step 2: Clean Data (Remove duplicates, empty entries, fix formatting)
def clean_data(extracted_data):
    """Cleans extracted data: removes duplicates, empty entries, and formats financial figures."""
    unique_data = []
    seen_entries = set()

    for entry in extracted_data:
        print(f"Processing entry: {entry}")  # Debugging print statement
        # Print keys and values for better understanding
        for key, value in entry.items():
            print(f"Key: {key}, Value: {value}")

        company = entry.get("company_name") or entry.get("Company Name")
        report_date = entry.get("report_date") or entry.get("Report Date")
        profit = entry.get("profit_before_tax") or entry.get("Profit Before Tax (PBT)") or entry.get("Profit Before Tax")

        # Skip empty or incomplete records
        if not all([company, report_date, profit]):
            continue

        key = (company, report_date, profit)
        if key not in seen_entries:
            seen_entries.add(key)

            # Convert Profit Before Tax to ‚ÇπX.XX crores format
            try:
                profit_value = float(profit)
                formatted_profit = f"‚Çπ{profit_value:.2f} crores"
            except ValueError:
                formatted_profit = profit  # If not a valid float, keep the original value

            unique_data.append({
                "Company Name": company,
                "Report Date": report_date,
                "Profit Before Tax": formatted_profit
            })

    print(f"‚úÖ Cleaned data: {len(unique_data)} unique records retained")
    return unique_data

# Step 3: Save Cleaned JSON Data
def save_json(data, output_path):
    """Saves cleaned data as a JSON file."""
    try:
        with open(output_path, "w", encoding="utf-8") as file:
            json.dump(data, file, indent=4)
        print(f"‚úÖ Cleaned data saved to: {output_path}")
    except Exception as e:
        print(f"‚ùå Error saving JSON: {e}")

# Main execution
extracted_data = load_json(input_file)  # Load extracted data
print(f"Extracted Data: {extracted_data}")  # Debugging print statement
cleaned_data = clean_data(extracted_data)  # Clean the data
save_json(cleaned_data, cleaned_file)  # Save cleaned data to cleaned.json

print("üöÄ Data cleaning complete. Results saved to cleaned.json")